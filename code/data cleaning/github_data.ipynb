{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from getpass import getuser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/single matches/\"\n",
    "website_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/website_df.xlsx\"\n",
    "website_df = pd.read_excel(website_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494, 19)\n",
      "(387, 19)\n",
      "(107, 19)\n"
     ]
    }
   ],
   "source": [
    "# Create the single_matches_web dataframe by selecting rows where p2t1 and p2t2 are empty\n",
    "single_matches_web_df = website_df[website_df['p2t1'].isna() & website_df['p2t2'].isna()]\n",
    "double_matches_web_df = website_df[website_df['p2t1'].notna() & website_df['p2t2'].notna()]\n",
    "print(website_df.shape)\n",
    "print(single_matches_web_df.shape)\n",
    "print(double_matches_web_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single_matches_web_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel Brands (Germany) vs Roberto Bautista Agut (Spain) is recorded as match number 4 in website_df since in the match before Feliciano Lopez (Spain) walked over Philipp Kohlschreiber (Germany). In website_df, we therefore consider it as a match not played whereas in github_df the number of match is preserved. We therefore change match number in website_df to make it consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of match  \n",
    "single_matches_web_df.loc[\n",
    "    (single_matches_web_df['year'] == 2014) &\n",
    "    (single_matches_web_df['team1'] == 'germany') &\n",
    "    (single_matches_web_df['team2'] == 'spain') &\n",
    "    (single_matches_web_df['match'] == 4), 'match'] = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# github data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of years to include\n",
    "years = range(2013, 2024)\n",
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    atp_matches_path = f\"{github_path}atp_matches_{year}.csv\"\n",
    "    df = pd.read_csv(atp_matches_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenating all the DataFrames into one\n",
    "atp_matches_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Extract the year from the 'tourney_id' by splitting the string at the first hyphen and taking the first part\n",
    "atp_matches_df['year'] = atp_matches_df['tourney_id'].str.split('-').str[0]\n",
    "\n",
    "# Convert the 'year' column to an integer\n",
    "atp_matches_df['year'] = atp_matches_df['year'].astype(int)\n",
    "\n",
    "# Replacing \"Frances Tiafoe\" with \"Franklin Tiafoe\" in both the 'winner_name' and 'loser_name' columns\n",
    "atp_matches_df['winner_name'] = atp_matches_df['winner_name'].replace('Frances Tiafoe', 'Franklin Tiafoe')\n",
    "atp_matches_df['loser_name'] = atp_matches_df['loser_name'].replace('Frances Tiafoe', 'Franklin Tiafoe')\n",
    "\n",
    "# Replacing \"Andreas Haider Maurer\" with \"Andreas Haider-Maurer\" in both the 'winner_name' and 'loser_name' columns\n",
    "atp_matches_df['winner_name'] = atp_matches_df['winner_name'].replace('Andreas Haider Maurer', 'Andreas Haider-Maurer')\n",
    "atp_matches_df['loser_name'] = atp_matches_df['loser_name'].replace('Andreas Haider Maurer', 'Andreas Haider-Maurer')\n",
    "\n",
    "# Replacing \"Guillermo Garcia Lopez\" with \"Guillermo Garcia-Lopez\" in both the 'winner_name' and 'loser_name' columns\n",
    "atp_matches_df['winner_name'] = atp_matches_df['winner_name'].replace('Guillermo Garcia Lopez', 'Guillermo Garcia-Lopez')\n",
    "atp_matches_df['loser_name'] = atp_matches_df['loser_name'].replace('Guillermo Garcia Lopez', 'Guillermo Garcia-Lopez')\n",
    "\n",
    "# Save the combined data to a single Excel file\n",
    "final_output_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/single matches/atp_matches_df.xlsx\"\n",
    "atp_matches_df.to_excel(final_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# github repo matched observations with website df players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store all ATP matches where Davis Cup players took part\n",
    "combined_matches_list = []\n",
    "\n",
    "# Iterate through each year to filter the matches\n",
    "for year in range(2013, 2024):\n",
    "    # Filter Davis Cup data for the current year\n",
    "    davis_single_year_df = single_matches_web_df[single_matches_web_df['year'] == year]\n",
    "    \n",
    "    # Create a set of tuples representing pairs of players for the current year\n",
    "    davis_pairs = set(zip(davis_single_year_df['p1t1'], davis_single_year_df['p1t2']))\n",
    "    \n",
    "    # Filter ATP matches data to match Davis Cup pairs for the current year\n",
    "    matched_atp_matches_df = atp_matches_df[\n",
    "        (atp_matches_df['year'] == year) &\n",
    "        atp_matches_df.apply(lambda row: (row['winner_name'], row['loser_name']) in davis_pairs or\n",
    "                                           (row['loser_name'], row['winner_name']) in davis_pairs, axis=1)\n",
    "    ]\n",
    "    \n",
    "    # Append the filtered data to the list\n",
    "    combined_matches_list.append(matched_atp_matches_df)\n",
    "\n",
    "# Concatenate all the DataFrames in the list into one combined DataFrame\n",
    "combined_matches_df = pd.concat(combined_matches_list, ignore_index=True)\n",
    "\n",
    "# Save the combined data to a single Excel file\n",
    "final_output_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/single matches/davis_cup_players.xlsx\"\n",
    "combined_matches_df.to_excel(final_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for davis cup matches from github 387\n",
      "Number of observations for matches played by davis cup players outside davis cup: 131\n"
     ]
    }
   ],
   "source": [
    "# Count the number of observations where tourney_level is \"D\" - Davis Cup\n",
    "davis_single_github_count = combined_matches_df[combined_matches_df['tourney_level'] == 'D'].shape[0]\n",
    "\n",
    "# Count the number of observations where tourney_level is not \"D\"\n",
    "extra_matches_single_count = combined_matches_df[combined_matches_df['tourney_level'] != 'D'].shape[0]\n",
    "\n",
    "print(\"Number of observations for davis cup matches from github\", davis_single_github_count)\n",
    "print(\"Number of observations for matches played by davis cup players outside davis cup:\", extra_matches_single_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "130 is the number of games played in other atp matches by davis cup players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# github dataset single match players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 50)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new DataFrame with observations where tourney_level is \"D\"\n",
    "single_github_df = combined_matches_df[combined_matches_df['tourney_level'] == 'D']\n",
    "single_github_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have 387 observations for single davis matches for both single_github_df and single_matches_web_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list from github dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2013, 1, 'Carlos Berlocq', 'Philipp Kohlschreiber'], [2013, 2, 'Florian Mayer', 'Juan Monaco'], [2013, 4, 'Juan Monaco', 'Tobias Kamke'], [2013, 5, 'Carlos Berlocq', 'Christopher Kas'], [2013, 1, 'David Goffin', 'Viktor Troicki']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a list with year, match_num, winner_name, loser_name, where names are alphabetically ordered\n",
    "github_list = single_github_df.apply(\n",
    "    lambda row: [row['year'],row['match_num'], *sorted([row['winner_name'], row['loser_name']])], \n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "# Optionally, print the first few elements in the list to verify\n",
    "print(github_list[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list from website df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2013, 1, 'Albert Ramos', 'Milos Raonic'], [2013, 2, 'Frank Dancevic', 'Marcel Granollers'], [2013, 4, 'Guillermo Garcia-Lopez', 'Milos Raonic'], [2013, 5, 'Albert Ramos', 'Frank Dancevic'], [2013, 1, 'Marin Cilic', 'Paolo Lorenzi']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Creating a list with Year, cleaned Match, P1T1, P1T2, where player names are alphabetically ordered\n",
    "website_list = single_matches_web_df.apply(\n",
    "    lambda row: [row['year'], row['match'], *sorted([row['p1t1'], row['p1t2']])],\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "# Optionally, print the first few elements in the list to verify\n",
    "print(website_list[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-matching elements in standardized_web_list: set()\n",
      "Non-matching elements in standardized_github_list: set()\n",
      "Number of unmatched elements in num_yes_github_no_web: 0\n",
      "Number of unmatched elements in num_no_github_yes_web: 0\n"
     ]
    }
   ],
   "source": [
    "# Standardizing formats in matches_list and davis_single_list\n",
    "# Assuming year and match number are integers, and player names are strings\n",
    "\n",
    "standardized_github_list = [\n",
    "    [int(year), int(match_num), str(winner_name), str(loser_name)]\n",
    "    for year, match_num, winner_name, loser_name in github_list\n",
    "]\n",
    "\n",
    "standardized_web_list = [\n",
    "    [int(year),int(match_num), str(player1), str(player2)]\n",
    "    for year, match_num, player1, player2 in website_list\n",
    "]\n",
    "\n",
    "# Convert to sets of tuples for comparison\n",
    "standardized_github_set = set(tuple(item) for item in standardized_github_list)\n",
    "standardized_web_set = set(tuple(item) for item in standardized_web_list)\n",
    "\n",
    "# Find non-matching elements in both sets\n",
    "yes_github_no_web = standardized_github_set - standardized_web_set\n",
    "no_github_yes_web = standardized_web_set - standardized_github_set\n",
    "\n",
    "print(\"Non-matching elements in standardized_web_list:\", yes_github_no_web)\n",
    "print(\"Non-matching elements in standardized_github_list:\", no_github_yes_web)\n",
    "\n",
    "# Count the number of non-matching tuples in standardized_github_list\n",
    "num_yes_github_no_web = len(yes_github_no_web)\n",
    "\n",
    "# Count the number of non-matching tuples in standardized_web_list\n",
    "num_no_github_yes_web = len(no_github_yes_web)\n",
    "\n",
    "print(\"Number of unmatched elements in num_yes_github_no_web:\", num_yes_github_no_web)\n",
    "print(\"Number of unmatched elements in num_no_github_yes_web:\", num_no_github_yes_web)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge two datasets single_github_df and single_matches_web_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched rows in combined_merged_df: 387\n",
      "Unmatched rows:\n",
      "                          tourney_id                     tourney_name surface  \\\n",
      "4                          2013-D003      Davis Cup WG R1: BEL vs SRB    Clay   \n",
      "5                          2013-D003      Davis Cup WG R1: BEL vs SRB    Clay   \n",
      "11                         2013-D001      Davis Cup WG R1: CAN vs ESP    Hard   \n",
      "16                         2013-D002      Davis Cup WG R1: ITA vs CRO    Clay   \n",
      "18                         2013-D002      Davis Cup WG R1: ITA vs CRO    Clay   \n",
      "..                               ...                              ...     ...   \n",
      "764  2023-M-DC-2023-FLS-M-CZE-AUS-01  Davis Cup Finals QF: CZE vs AUS    Hard   \n",
      "769  2023-M-DC-2023-FLS-M-ITA-NED-01  Davis Cup Finals QF: ITA vs NED    Hard   \n",
      "771  2023-M-DC-2023-FLS-M-ITA-SRB-01  Davis Cup Finals SF: ITA vs SRB    Hard   \n",
      "772  2023-M-DC-2023-FLS-M-SRB-GBR-01  Davis Cup Finals QF: SRB vs GBR    Hard   \n",
      "773  2023-M-DC-2023-FLS-M-SRB-GBR-01  Davis Cup Finals QF: SRB vs GBR    Hard   \n",
      "\n",
      "     draw_size tourney_level  tourney_date  match_num  winner_id  winner_seed  \\\n",
      "4            4             D      20130201          1     104678          NaN   \n",
      "5            4             D      20130201          2     104925          NaN   \n",
      "11           4             D      20130201          5     105077          NaN   \n",
      "16           4             D      20130201          1     105227          NaN   \n",
      "18           4             D      20130201          4     105227          NaN   \n",
      "..         ...           ...           ...        ...        ...          ...   \n",
      "764          2             D      20231122          1     207830          NaN   \n",
      "769          2             D      20231123          2     206173          NaN   \n",
      "771          2             D      20231125          2     206173          NaN   \n",
      "772          2             D      20231123          1     200175          NaN   \n",
      "773          2             D      20231123          2     104925          NaN   \n",
      "\n",
      "    winner_entry  ... date_start date_end  venue_name city  country  \\\n",
      "4            NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "5            NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "11           NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "16           NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "18           NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "..           ...  ...        ...      ...         ...  ...      ...   \n",
      "764          NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "769          NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "771          NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "772          NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "773          NaN  ...        NaN      NaN         NaN  NaN      NaN   \n",
      "\n",
      "     main_surface  specific_surface indoor_outdoor court_pace_rating  \\\n",
      "4             NaN               NaN            NaN               NaN   \n",
      "5             NaN               NaN            NaN               NaN   \n",
      "11            NaN               NaN            NaN               NaN   \n",
      "16            NaN               NaN            NaN               NaN   \n",
      "18            NaN               NaN            NaN               NaN   \n",
      "..            ...               ...            ...               ...   \n",
      "764           NaN               NaN            NaN               NaN   \n",
      "769           NaN               NaN            NaN               NaN   \n",
      "771           NaN               NaN            NaN               NaN   \n",
      "772           NaN               NaN            NaN               NaN   \n",
      "773           NaN               NaN            NaN               NaN   \n",
      "\n",
      "        _merge  \n",
      "4    left_only  \n",
      "5    left_only  \n",
      "11   left_only  \n",
      "16   left_only  \n",
      "18   left_only  \n",
      "..         ...  \n",
      "764  left_only  \n",
      "769  left_only  \n",
      "771  left_only  \n",
      "772  left_only  \n",
      "773  left_only  \n",
      "\n",
      "[387 rows x 69 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_17836\\3206966950.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_github_df['year'] = single_github_df['year'].astype(int)\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_17836\\3206966950.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_matches_web_df['year'] = single_matches_web_df['year'].astype(int)\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_17836\\3206966950.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_github_df['winner_name'] = single_github_df['winner_name'].str.lower().str.strip()\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_17836\\3206966950.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_github_df['loser_name'] = single_github_df['loser_name'].str.lower().str.strip()\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_17836\\3206966950.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_matches_web_df['p1t1'] = single_matches_web_df['p1t1'].str.lower().str.strip()\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_17836\\3206966950.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_matches_web_df['p1t2'] = single_matches_web_df['p1t2'].str.lower().str.strip()\n"
     ]
    }
   ],
   "source": [
    "# Ensure 'year' is an integer in single_github_df\n",
    "single_github_df['year'] = single_github_df['year'].astype(int)\n",
    "\n",
    "# Ensure 'year' is an integer in single_matches_web_df\n",
    "single_matches_web_df['year'] = single_matches_web_df['year'].astype(int)\n",
    "\n",
    "# Standardize case and remove leading/trailing spaces in single_github_df\n",
    "single_github_df['winner_name'] = single_github_df['winner_name'].str.lower().str.strip()\n",
    "single_github_df['loser_name'] = single_github_df['loser_name'].str.lower().str.strip()\n",
    "\n",
    "# Standardize case and remove leading/trailing spaces in single_matches_web_df\n",
    "single_matches_web_df['p1t1'] = single_matches_web_df['p1t1'].str.lower().str.strip()\n",
    "single_matches_web_df['p1t2'] = single_matches_web_df['p1t2'].str.lower().str.strip()\n",
    "\n",
    "# Now perform the merges as described\n",
    "\n",
    "# Direct match merge\n",
    "merged_df = pd.merge(\n",
    "    single_github_df, \n",
    "    single_matches_web_df, \n",
    "    how='left', \n",
    "    left_on=['year', 'winner_name', 'loser_name'], \n",
    "    right_on=['year', 'p1t1', 'p1t2'],\n",
    "    indicator=True  # This will add a column to indicate if the merge was successful\n",
    ")\n",
    "\n",
    "# Reverse match merge (loser_name vs. winner_name)\n",
    "merged_df_reverse = pd.merge(\n",
    "    single_github_df, \n",
    "    single_matches_web_df, \n",
    "    how='left', \n",
    "    left_on=['year', 'loser_name', 'winner_name'], \n",
    "    right_on=['year', 'p1t1', 'p1t2'],\n",
    "    indicator=True  # This will add a column to indicate if the merge was successful\n",
    ")\n",
    "\n",
    "# Combine both merged DataFrames\n",
    "combined_merged_df = pd.concat([merged_df, merged_df_reverse], ignore_index=True)\n",
    "\n",
    "# Drop duplicates if any\n",
    "combined_merged_df = combined_merged_df.drop_duplicates()\n",
    "\n",
    "# Check for unmatched rows in combined_merged_df\n",
    "unmatched_rows_df = combined_merged_df[combined_merged_df['_merge'] == 'left_only']\n",
    "\n",
    "# Count the number of unmatched rows\n",
    "num_unmatched = unmatched_rows_df.shape[0]\n",
    "\n",
    "print(f\"Number of unmatched rows in combined_merged_df: {num_unmatched}\")\n",
    "\n",
    "# If needed, you can inspect these unmatched rows\n",
    "if num_unmatched > 0:\n",
    "    print(\"Unmatched rows:\")\n",
    "    print(unmatched_rows_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exported to C:/Users/aldi/Documents/GitHub/davis-cup/data/unmatched_rows_df.xlsx\n",
      "DataFrame exported to C:/Users/aldi/Documents/GitHub/davis-cup/data/single_matches_web.xlsx\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/unmatched_rows_df.xlsx\"\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "unmatched_rows_df.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame exported to {file_path}\")\n",
    "\n",
    "file_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/single_matches_web.xlsx\"\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "single_matches_web.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame exported to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtered_atp_matches.xlsx in single matches folder = final df for single matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matches found or 'tourney_id' column missing in all years.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample setup - ensure to replace this with actual data loading logic\n",
    "# df_paired = pd.read_csv('path_to_davis_cup_data.csv')\n",
    "\n",
    "folder_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/double matches/\"\n",
    "\n",
    "double_matches_df = pd.DataFrame()\n",
    "\n",
    "for year in range(2014, 2021):\n",
    "    davis_double_year_df = davis_double_df[davis_double_df['year'] == year].copy()\n",
    "\n",
    "    # Generate sorted tuples for each Davis Cup match\n",
    "    davis_quads = set(\n",
    "        davis_double_year_df.apply(\n",
    "            lambda x: (\n",
    "                tuple(sorted([str(x['p1t1']), str(x['p2t1'])])),\n",
    "                tuple(sorted([str(x['p1t2']), str(x['p2t2'])]))\n",
    "            ), axis=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    atp_matches_path = f\"{folder_path}atp_matches_doubles_{year}.csv\"\n",
    "    atp_matches_df = pd.read_csv(atp_matches_path)\n",
    "\n",
    "    # Filter out any rows that might have NaN values for player names to avoid type errors\n",
    "    atp_matches_df.dropna(subset=['winner1_name', 'winner2_name', 'loser1_name', 'loser2_name'], inplace=True)\n",
    "\n",
    "    # Create sorted tuples for each ATP match\n",
    "    matched_atp_matches_df = atp_matches_df[\n",
    "        atp_matches_df.apply(\n",
    "            lambda row: (\n",
    "                tuple(sorted([str(row['winner1_name']), str(row['winner2_name'])])),\n",
    "                tuple(sorted([str(row['loser1_name']), str(row['loser2_name'])]))\n",
    "            ) in davis_quads, axis=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Append matched ATP matches to the final DataFrame\n",
    "    double_matches_df = pd.concat([double_matches_df, matched_atp_matches_df], ignore_index=True)\n",
    "\n",
    "if not double_matches_df.empty:\n",
    "    double_matches_df['year'] = double_matches_df['tourney_id'].astype(str).str.split('-').str[0].astype(int)\n",
    "    final_output_path = \"C:/Users/{}/Documents/GitHub/davis-cup/data/double matches/filtered_atp_matches_doubles.xlsx\"\n",
    "    double_matches_df.to_excel(final_output_path, index=False)\n",
    "else:\n",
    "    print(\"No matches found or 'tourney_id' column missing in all years.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
