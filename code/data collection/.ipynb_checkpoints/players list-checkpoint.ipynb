{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "user = \"aldi\"  # Replace with the actual username\n",
    "file_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/combined_davis.xlsx\"  \n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Loop through the DataFrame and mark rows for deletion\n",
    "to_delete = []\n",
    "for i in range(len(df) - 1):\n",
    "    # Check for consecutive rows with \"Sam Groth\" as \"Player 1\" and \"John Peers\" as \"Player 2\"\n",
    "    if (df.iloc[i]['Player 1'] == \"Sam Groth\" and \n",
    "        df.iloc[i]['Player 2'] == \"John Peers\" and\n",
    "        df.iloc[i]['Player 1'] == df.iloc[i + 1]['Player 1'] and\n",
    "        df.iloc[i]['Player 2'] == df.iloc[i + 1]['Player 2']):\n",
    "        to_delete.append(i + 1)\n",
    "    \n",
    "    # Check for consecutive rows with \"Jordan Thompson\" as \"Player 1\"\n",
    "    elif (df.iloc[i]['Player 1'] == \"Jordan Thompson\" and \n",
    "          df.iloc[i]['Player 1'] == df.iloc[i + 1]['Player 1']):\n",
    "        to_delete.append(i + 1)\n",
    "\n",
    "    # Check for consecutive rows with \"Nick Kyrgios\" as \"Player 1\"\n",
    "    elif (df.iloc[i]['Player 1'] == \"Nick Kyrgios\" and \n",
    "          df.iloc[i]['Player 1'] == df.iloc[i + 1]['Player 1']):\n",
    "        to_delete.append(i + 1)\n",
    "        \n",
    "    # Check for consecutive rows with \"Sam Groth\" as \"Player 1\"\n",
    "    elif (df.iloc[i]['Player 1'] == \"Sam Groth\" and \n",
    "          df.iloc[i]['Player 1'] == df.iloc[i + 1]['Player 1']):\n",
    "        to_delete.append(i + 1)\n",
    "    # Check for consecutive rows with \"Sam Groth\" as \"Player 1\" and \"John Peers\" as \"Player 2\"\n",
    "    elif (df.iloc[i]['Player 1'] == \"John Peers\" and \n",
    "        df.iloc[i]['Player 2'] == \"Jordan Thompson\" and\n",
    "        df.iloc[i]['Player 1'] == df.iloc[i + 1]['Player 1'] and\n",
    "        df.iloc[i]['Player 2'] == df.iloc[i + 1]['Player 2']):\n",
    "        to_delete.append(i + 1)\n",
    "\n",
    "\n",
    "# Remove the marked rows\n",
    "df_cleaned = df.drop(df.index[to_delete])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (866, 28)\n",
      "df_cleaned shape: (846, 28)\n",
      "df_paired shape: (423, 10)\n"
     ]
    }
   ],
   "source": [
    "# Extract the relevant columns and rename them\n",
    "df_extracted = df_cleaned[['Stage', 'Player 1', 'Player 2', 'Year', 'Date', 'Venue', 'Court Pace Rating', 'Ball', 'match']].copy()\n",
    "df_extracted.rename(columns={'Player 1': 'P1T1', 'Player 2': 'P2T1'}, inplace=True)\n",
    "\n",
    "# Helper function to pair rows with the additional condition that \"Date\" should be equal\n",
    "def pair_rows(df):\n",
    "    paired_rows = []\n",
    "    for i in range(0, len(df), 2):\n",
    "        if i+1 < len(df):\n",
    "            row1 = df.iloc[i]\n",
    "            row2 = df.iloc[i+1]\n",
    "            if row1['match'] == row2['match'] and row1['Year'] == row2['Year'] and row1['Date'] == row2['Date']:\n",
    "                paired_row = [row1['match'], row1['Year'], row1['P1T1'], row1['P2T1'], row2['P1T1'], row2['P2T1'], row1['Date'], row1['Venue'], row1['Court Pace Rating'], row1['Ball']]\n",
    "                paired_rows.append(paired_row)\n",
    "    paired_df = pd.DataFrame(paired_rows, columns=['Match', 'Year', 'P1T1', 'P2T1', 'P1T2', 'P2T2', 'Date', 'Venue', 'Court Pace Rating', 'Ball'])\n",
    "    return paired_df\n",
    "\n",
    "# Pair the rows in the extracted dataframe\n",
    "df_paired = pair_rows(df_extracted)\n",
    "\n",
    "# Replace specific player names to match with data from atp matches\n",
    "df_paired.replace({\n",
    "    'P1T1': {\n",
    "        'Jan-Lennard Struff': 'Jan Lennard Struff',\n",
    "        'Juan Martin Del Potro': 'Juan Martin del Potro',\n",
    "        'Albert Ramos-Vinolas': 'Albert Ramos',\n",
    "        'Felix Auger-Aliassime': 'Felix Auger Aliassime',\n",
    "        'Pierre-Hugues Herbert': 'Pierre Hugues Herbert',\n",
    "        'Marc-Andrea Huesler': 'Marc Andrea Huesler',\n",
    "        'Roman Khassanov': 'Roman Hassanov'\n",
    "    },\n",
    "    'P2T1': {\n",
    "        'Jan-Lennard Struff': 'Jan Lennard Struff',\n",
    "        'Juan Martin Del Potro': 'Juan Martin del Potro',\n",
    "        'Albert Ramos-Vinolas': 'Albert Ramos',\n",
    "        'Felix Auger-Aliassime': 'Felix Auger Aliassime',\n",
    "        'Pierre-Hugues Herbert': 'Pierre Hugues Herbert',\n",
    "        'Marc-Andrea Huesler': 'Marc Andrea Huesler',\n",
    "        'Roman Khassanov': 'Roman Hassanov'\n",
    "\n",
    "    },\n",
    "    'P1T2': {\n",
    "        'Jan-Lennard Struff': 'Jan Lennard Struff',\n",
    "        'Juan Martin Del Potro': 'Juan Martin del Potro',\n",
    "        'Albert Ramos-Vinolas': 'Albert Ramos',\n",
    "        'Felix Auger-Aliassime': 'Felix Auger Aliassime',\n",
    "         'Pierre-Hugues Herbert': 'Pierre Hugues Herbert',\n",
    "        'Marc-Andrea Huesler': 'Marc Andrea Huesler',\n",
    "        'Roman Khassanov': 'Roman Hassanov'\n",
    "\n",
    "    },\n",
    "    'P2T2': {\n",
    "        'Jan-Lennard Struff': 'Jan Lennard Struff',\n",
    "        'Juan Martin Del Potro': 'Juan Martin del Potro',\n",
    "        'Albert Ramos-Vinolas': 'Albert Ramos',\n",
    "        'Felix Auger-Aliassime': 'Felix Auger Aliassime',\n",
    "         'Pierre-Hugues Herbert': 'Pierre Hugues Herbert',\n",
    "        'Marc-Andrea Huesler': 'Marc Andrea Huesler',\n",
    "        'Roman Khassanov': 'Roman Hassanov'\n",
    "\n",
    "    }\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Print the shapes of the dataframes\n",
    "print(\"df shape:\", df.shape)\n",
    "print(\"df_cleaned shape:\", df_cleaned.shape)\n",
    "print(\"df_paired shape:\", df_paired.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All matches are in the correct range.\n"
     ]
    }
   ],
   "source": [
    "# Group by Date, Year, and Venue, then count observations\n",
    "grouped_counts = df_cleaned.groupby(['Date', 'Year', 'Venue']).size()\n",
    "\n",
    "# Define a function to check if counts meet the specified criteria\n",
    "def check_count(year, count):\n",
    "    if year <= 2018:\n",
    "        return count == 8 or count == 10\n",
    "    else:\n",
    "        return 4 <= count <= 16\n",
    "\n",
    "# Apply the function to each row in the grouped data and create a new column 'Count'\n",
    "correct_counts_df = grouped_counts.reset_index()\n",
    "correct_counts_df['Count'] = correct_counts_df.apply(lambda row: row[0], axis=1)\n",
    "\n",
    "# Filter rows where counts do not meet the criteria\n",
    "incorrect_counts_df = correct_counts_df[~correct_counts_df.apply(lambda row: check_count(row['Year'], row['Count']), axis=1)]\n",
    "\n",
    "# Check if the DataFrame is empty, indicating all matches are in the correct range\n",
    "if incorrect_counts_df.empty:\n",
    "    print(\"All matches are in the correct range.\")\n",
    "else:\n",
    "    # Display Date, Year, Venue, and Count when correct_counts is False\n",
    "    print(incorrect_counts_df[['Date', 'Year', 'Venue', 'Count']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check correspondance between data official website (davis matches) and https://github.com/JeffSackmann/tennis_atp/tree/master repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davis_single_df (331, 10)\n"
     ]
    }
   ],
   "source": [
    "# keep only rows for single matches\n",
    "davis_single_df = df_paired[df_paired['P2T1'].isna() & df_paired['P2T2'].isna()]\n",
    "print(\"davis_single_df\", davis_single_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match</th>\n",
       "      <th>Year</th>\n",
       "      <th>P1T1</th>\n",
       "      <th>P2T1</th>\n",
       "      <th>P1T2</th>\n",
       "      <th>P2T2</th>\n",
       "      <th>Date</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Court Pace Rating</th>\n",
       "      <th>Ball</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>MATCH 1</td>\n",
       "      <td>2016</td>\n",
       "      <td>Andy Murray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Juan Martin del Potro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16 Sep - 18 Sep 2016</td>\n",
       "      <td>Emirates Arena, Glasgow, Great Britain</td>\n",
       "      <td>Medium Fast</td>\n",
       "      <td>Slazenger Wimbledon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>MATCH 2</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ivo Karlovic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Juan Martin del Potro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25 Nov - 27 Nov 2016</td>\n",
       "      <td>Arena Zagreb, Zagreb, Croatia</td>\n",
       "      <td>Medium Slow</td>\n",
       "      <td>Wilson US Open Extra Duty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>MATCH 4</td>\n",
       "      <td>2016</td>\n",
       "      <td>Marin Cilic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Juan Martin del Potro</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25 Nov - 27 Nov 2016</td>\n",
       "      <td>Arena Zagreb, Zagreb, Croatia</td>\n",
       "      <td>Medium Slow</td>\n",
       "      <td>Wilson US Open Extra Duty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Match  Year          P1T1 P2T1                   P1T2 P2T2  \\\n",
       "195  MATCH 1  2016   Andy Murray  NaN  Juan Martin del Potro  NaN   \n",
       "206  MATCH 2  2016  Ivo Karlovic  NaN  Juan Martin del Potro  NaN   \n",
       "208  MATCH 4  2016   Marin Cilic  NaN  Juan Martin del Potro  NaN   \n",
       "\n",
       "                      Date                                    Venue  \\\n",
       "195   16 Sep - 18 Sep 2016   Emirates Arena, Glasgow, Great Britain   \n",
       "206   25 Nov - 27 Nov 2016            Arena Zagreb, Zagreb, Croatia   \n",
       "208   25 Nov - 27 Nov 2016            Arena Zagreb, Zagreb, Croatia   \n",
       "\n",
       "    Court Pace Rating                        Ball  \n",
       "195       Medium Fast         Slazenger Wimbledon  \n",
       "206       Medium Slow   Wilson US Open Extra Duty  \n",
       "208       Medium Slow   Wilson US Open Extra Duty  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davis_single_df.loc[davis_single_df['P1T2'] == 'Juan Martin del Potro',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/single matches/\"\n",
    "\n",
    "# Initialize DataFrame to store all ATP matches where Davis Cup players took part\n",
    "combined_matches_df = pd.DataFrame()\n",
    "\n",
    "for year in range(2014, 2024):\n",
    "    # Filter Davis Cup data for the current year\n",
    "    davis_single_year_df = df_paired[df_paired['Year'] == year]\n",
    "\n",
    "    # Create a set of tuples representing pairs of players for the current year\n",
    "    davis_pairs = set(zip(davis_single_year_df['P1T1'], davis_single_year_df['P1T2']))\n",
    "\n",
    "    # Load ATP matches data for the current year\n",
    "    atp_matches_path = f\"{folder_path}atp_matches_{year}.csv\"\n",
    "    atp_matches_df = pd.read_csv(atp_matches_path)\n",
    "\n",
    "    # Filter ATP matches data to match Davis Cup pairs\n",
    "    matched_atp_matches_df = atp_matches_df[\n",
    "        atp_matches_df.apply(lambda row: (row['winner_name'], row['loser_name']) in davis_pairs or\n",
    "                                           (row['loser_name'], row['winner_name']) in davis_pairs, axis=1)\n",
    "    ]\n",
    "\n",
    "    # Append the filtered data to the combined DataFrame\n",
    "    combined_matches_df = combined_matches_df.append(matched_atp_matches_df)\n",
    "\n",
    "# # Extract the year information before the first hyphen in 'tourney_id'\n",
    "combined_matches_df['year'] = combined_matches_df['tourney_id'].str.split('-').str[0]\n",
    "\n",
    "# Convert 'year' column to integer\n",
    "combined_matches_df['year'] = combined_matches_df['year'].astype(int)\n",
    "\n",
    "# Save the combined data to a single Excel file\n",
    "final_output_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/single matches/filtered_atp_matches.xlsx\"\n",
    "combined_matches_df.to_excel(final_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations with tourney_level 'D': 331\n",
      "Number of observations without tourney_level 'D': 117\n"
     ]
    }
   ],
   "source": [
    "# Count the number of observations where tourney_level is \"D\" - Davis Cup\n",
    "count_tourney_level_D = combined_matches_df[combined_matches_df['tourney_level'] == 'D'].shape[0]\n",
    "\n",
    "# Count the number of observations where tourney_level is not \"D\"\n",
    "count_not_tourney_level_D = combined_matches_df[combined_matches_df['tourney_level'] != 'D'].shape[0]\n",
    "\n",
    "print(\"Number of observations with tourney_level 'D':\", count_tourney_level_D)\n",
    "print(\"Number of observations without tourney_level 'D':\", count_not_tourney_level_D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "331 corresponds in both datasets combined_matches_df and davis_single_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "117 is the number of games played in other atp matches by davis cup players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new DataFrame with observations where tourney_level is \"D\"\n",
    "davis_cup_matches_df = combined_matches_df[combined_matches_df['tourney_level'] == 'D']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2014, 1, 'Andreas Seppi', 'Carlos Berlocq'], [2014, 2, 'Fabio Fognini', 'Juan Monaco'], [2014, 4, 'Carlos Berlocq', 'Fabio Fognini'], [2014, 1, 'Radek Stepanek', 'Robin Haase'], [2014, 2, 'Igor Sijsling', 'Tomas Berdych']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a list with year, match_num, winner_name, loser_name, where names are alphabetically ordered\n",
    "matches_list = davis_cup_matches_df.apply(\n",
    "    lambda row: [row['year'],row['match_num'], *sorted([row['winner_name'], row['loser_name']])], \n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "# Optionally, print the first few elements in the list to verify\n",
    "print(matches_list[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2014, 1, 'Radek Stepanek', 'Robin Haase'], [2014, 2, 'Igor Sijsling', 'Tomas Berdych'], [2014, 4, 'Thiemo De Bakker', 'Tomas Berdych'], [2014, 5, 'Igor Sijsling', 'Lukas Rosol'], [2014, 1, 'Kei Nishikori', 'Peter Polansky']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Assuming davis_single_df is your DataFrame\n",
    "\n",
    "# Convert 'Match' column to string type first, then clean it to keep only the number after \"MATCH\"\n",
    "davis_single_df['Match'] = davis_single_df['Match'].astype(str).str.replace('MATCH ', '').astype(int)\n",
    "\n",
    "# Creating a list with Year, cleaned Match, P1T1, P1T2, where player names are alphabetically ordered\n",
    "davis_single_list = davis_single_df.apply(\n",
    "    lambda row: [row['Year'], row['Match'], *sorted([row['P1T1'], row['P1T2']])],\n",
    "    axis=1\n",
    ").tolist()\n",
    "\n",
    "# Optionally, print the first few elements in the list to verify\n",
    "print(davis_single_list[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-matching elements in standardized_matches_list: {(2014, 5, 'Daniel Brands', 'Roberto Bautista Agut')}\n",
      "Non-matching elements in standardized_davis_single_list: {(2014, 4, 'Daniel Brands', 'Roberto Bautista Agut')}\n",
      "Number of unmatched elements in standardized_matches_list: 1\n",
      "Number of unmatched elements in standardized_davis_single_list: 1\n"
     ]
    }
   ],
   "source": [
    "# Standardizing formats in matches_list and davis_single_list\n",
    "# Assuming year and match number are integers, and player names are strings\n",
    "\n",
    "standardized_matches_list = [\n",
    "    [int(year), int(match_num), str(winner_name), str(loser_name)]\n",
    "    for year, match_num, winner_name, loser_name in matches_list\n",
    "]\n",
    "\n",
    "standardized_davis_single_list = [\n",
    "    [int(year),int(match_num), str(player1), str(player2)]\n",
    "    for year, match_num, player1, player2 in davis_single_list\n",
    "]\n",
    "\n",
    "# Convert to sets of tuples for comparison\n",
    "standardized_matches_set = set(tuple(item) for item in standardized_matches_list)\n",
    "standardized_davis_single_set = set(tuple(item) for item in standardized_davis_single_list)\n",
    "\n",
    "# Find non-matching elements in both sets\n",
    "non_matching_in_standardized_matches = standardized_matches_set - standardized_davis_single_set\n",
    "non_matching_in_standardized_davis_single = standardized_davis_single_set - standardized_matches_set\n",
    "\n",
    "print(\"Non-matching elements in standardized_matches_list:\", non_matching_in_standardized_matches)\n",
    "print(\"Non-matching elements in standardized_davis_single_list:\", non_matching_in_standardized_davis_single)\n",
    "\n",
    "# Count the number of non-matching tuples in standardized_matches_list\n",
    "num_unmatched_in_standardized_matches = len(non_matching_in_standardized_matches)\n",
    "\n",
    "# Count the number of non-matching tuples in standardized_davis_single_list\n",
    "num_unmatched_in_standardized_davis_single = len(non_matching_in_standardized_davis_single)\n",
    "\n",
    "print(\"Number of unmatched elements in standardized_matches_list:\", num_unmatched_in_standardized_matches)\n",
    "print(\"Number of unmatched elements in standardized_davis_single_list:\", num_unmatched_in_standardized_davis_single)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep rows where P1T1, P2T1, P1T2, P2T2 are all non-NA\n",
    "davis_double_df = df_paired.dropna(subset=['P1T1', 'P2T1', 'P1T2', 'P2T2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "davis_double_df (92, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"davis_double_df\", davis_double_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/aldi/Documents/GitHub/davis-cup/data/double matches/atp_matches_double_2014.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-cbabe85ea5af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Load ATP doubles matches data for the current year\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0matp_matches_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{folder_path}atp_matches_double_{year}.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0matp_matches_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matp_matches_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Filter ATP matches data to match Davis Cup quadruples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoding_errors\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         )\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    708\u001b[0m             )\n\u001b[0;32m    709\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/aldi/Documents/GitHub/davis-cup/data/double matches/atp_matches_double_2014.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adjusted folder path for double matches\n",
    "folder_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/double matches/\"\n",
    "\n",
    "# Initialize DataFrame to store all ATP double matches where Davis Cup players took part\n",
    "double_matches_df = pd.DataFrame()\n",
    "\n",
    "for year in range(2014, 2024):\n",
    "    # Filter Davis Cup data for the current year for doubles\n",
    "    davis_double_year_df = df_paired[df_paired['Year'] == year]\n",
    "\n",
    "    # Create a set of tuples representing pairs of players for the current year (for doubles, we need quadruples)\n",
    "    davis_quads = set(zip(davis_double_year_df['P1T1'], davis_double_year_df['P2T1'], davis_double_year_df['P1T2'], davis_double_year_df['P2T2']))\n",
    "\n",
    "    # Load ATP doubles matches data for the current year\n",
    "    atp_matches_path = f\"{folder_path}atp_matches_double_{year}.csv\"\n",
    "    atp_matches_df = pd.read_csv(atp_matches_path)\n",
    "\n",
    "    # Filter ATP matches data to match Davis Cup quadruples\n",
    "    matched_atp_matches_df = atp_matches_df[\n",
    "        atp_matches_df.apply(lambda row: (row['winner1_name'], row['winner2_name'], row['loser1_name'], row['loser2_name']) in davis_quads or\n",
    "                                           (row['loser1_name'], row['loser2_name'], row['winner1_name'], row['winner2_name']) in davis_quads, axis=1)\n",
    "    ]\n",
    "\n",
    "    # Append the filtered data to the double matches DataFrame\n",
    "    double_matches_df = double_matches_df.append(matched_atp_matches_df)\n",
    "\n",
    "# Extract the year information before the first hyphen in 'tourney_id'\n",
    "double_matches_df['year'] = double_matches_df['tourney_id'].str.split('-').str[0]\n",
    "\n",
    "# Convert 'year' column to integer\n",
    "double_matches_df['year'] = double_matches_df['year'].astype(int)\n",
    "\n",
    "# Save the combined data to a single Excel file for doubles\n",
    "final_output_path = \"C:/Users/{}/Documents/GitHub/davis-cup/data/double matches/filtered_atp_matches_doubles.xlsx\"\n",
    "double_matches_df.to_excel(final_output_path, index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names in combined_matches_df\n",
    "print(combined_matches_df.columns)\n",
    "\n",
    "# If you have already created single_davis_df, check its columns too\n",
    "print(single_davis_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"C:/Users/{user}/Documents/GitHub/davis-cup/data/singles_paired.xlsx\"\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df_paired.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame exported to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtered_atp_matches.xlsx in single matches folder = final df for single matches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
